# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jyJBYvE0fquY7YI1Qae0QH451h4rsUpX
"""

block_size = 64
batch_size = 32
embed_dim = 128
heads = 4
layers = 4
ff_dim = 512
steps = 2000

import os
import torch
import torch.nn as nn

def load_text(filename="data.txt"):
    with open(filename, "r", encoding="utf-8") as f:
        return f.read()

# tokenizer.py
class CharTokenizer:
    def __init__(self, text):
        self.chars = sorted(list(set(text)))
        self.stoi = {ch: i for i, ch in enumerate(self.chars)}
        self.itos = {i: ch for ch, i in self.stoi.items()}
        self.vocab_size = len(self.chars)

    def encode(self, s):
        return [self.stoi[c] for c in s]

    def decode(self, tokens):
        return ''.join([self.itos[i] for i in tokens])

import torch
import torch.nn as nn

class GPTModel(nn.Module):
    def __init__(self, vocab_size, max_len, dim, heads, layers, ff_dim):
        super().__init__()
        self.token_embed = nn.Embedding(vocab_size, dim)
        self.pos_embed = nn.Embedding(max_len, dim)
        self.blocks = nn.ModuleList([
            TransformerBlock(dim, heads, ff_dim) for _ in range(layers)
        ])
        self.norm = nn.LayerNorm(dim)
        self.lm_head = nn.Linear(dim, vocab_size)

    def forward(self, x):
        B, T = x.shape
        pos = torch.arange(0, T, device=x.device).unsqueeze(0)
        x = self.token_embed(x) + self.pos_embed(pos)

        # Causal mask to ensure no attending to future tokens
        mask = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0).unsqueeze(0)

        for block in self.blocks:
            x = block(x, mask)

        x = self.norm(x)
        return self.lm_head(x)

class TransformerBlock(nn.Module):
    def __init__(self, dim, heads, ff_dim):
        super().__init__()
        self.attn = MultiHeadSelfAttention(dim, heads)
        self.ff = FeedForward(dim, ff_dim)
        self.norm1 = nn.LayerNorm(dim)
        self.norm2 = nn.LayerNorm(dim)

    def forward(self, x, mask=None):
        x = x + self.attn(self.norm1(x))
        x = x + self.ff(self.norm2(x))
        return x

class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        assert embed_dim % num_heads == 0
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        self.qkv = nn.Linear(embed_dim, 3 * embed_dim)
        self.out = nn.Linear(embed_dim, embed_dim)
        self.scale = self.head_dim ** -0.5

    def forward(self, x, mask=None):
        B, T, C = x.size()
        qkv = self.qkv(x).chunk(3, dim=-1)
        q, k, v = [t.view(B, T, self.num_heads, self.head_dim).transpose(1, 2) for t in qkv]

        attn_scores = (q @ k.transpose(-2, -1)) * self.scale
        if mask is not None:
            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))
        attn_probs = torch.softmax(attn_scores, dim=-1)
        out = attn_probs @ v
        out = out.transpose(1, 2).contiguous().view(B, T, C)
        return self.out(out)

class FeedForward(nn.Module):
    def __init__(self, dim, hidden_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, dim)
        )

    def forward(self, x):
        return self.net(x)

text = load_text("data.txt")
tok = CharTokenizer(text)
data = tok.encode(text)
split = int(0.9 * len(data))
train_data, val_data = data[:split], data[split:]

model = GPTModel(tok.vocab_size, block_size, embed_dim, heads, layers, ff_dim)
optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)
loss_fn = torch.nn.CrossEntropyLoss()

def get_batch(data, block_size, batch_size):
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.tensor([data[i:i+block_size] for i in ix])
    y = torch.tensor([data[i+1:i+block_size+1] for i in ix])
    return x, y

model.train()
for step in range(steps):
    xb, yb = get_batch(train_data, block_size, batch_size)
    logits = model(xb)
    loss = loss_fn(logits.view(-1, tok.vocab_size), yb.view(-1))

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if step % 100 == 0:
        print(f"Step {step} | Loss: {loss.item():.4f}")

torch.save(model.state_dict(), "trained_gpt_model.pt")
print("âœ… Model saved successfully!")

# generate.py
import torch

@torch.no_grad()
def generate(model, tok, prompt, max_new_tokens=100):
    model.eval()
    input_ids = torch.tensor([tok.encode(prompt)], dtype=torch.long)
    for _ in range(max_new_tokens):
        input_cond = input_ids[:, -model.pos_embed.num_embeddings:]
        logits = model(input_cond)
        next_token = torch.argmax(logits[:, -1, :], dim=-1)
        input_ids = torch.cat((input_ids, next_token.unsqueeze(1)), dim=1)
    return tok.decode(input_ids[0].tolist())

